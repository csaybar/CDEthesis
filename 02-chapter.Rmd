---
output:
  bookdown::html_document2: default
  bookdown::pdf_document2:
template: templates/brief_template.tex
citation_package: biblatex
bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---
  
```{block type='savequote', quote_author='(ref:lesly02)', include=knitr::is_latex_output()}
What am I in the eyes of most people - a nonentity, an eccentric or an unpleasant person - somebody who has no position in society and never will have, in short, the lowest of the low. All right, then - even if that were absolutely true, then I should one day like to show by my work what such an eccentric, such a nobody, has in his heart.
```
(ref:lesly02) --- Vincent Van Gogh - 1882

# Uncertainty estimation
\minitoc <!-- this will include a mini table of contents-->
  
## Introduction
  
Cloud masking is an essential pre-processing for any application of optical remote sensing imagery. Various coarse-resolution cloud cover datasets (\cite{sassen2008classifying, winker2010calipso, Wilson2016}) estimate the worldwide multi-annual cloud occurrence percentage to be around 0.6 ± 0.2, with hotspots in tropical and subtropical forests. Assuming these products are indicative of Sentinel-2 cloud conditions, we can anticipate that more than half of the pixels will need to be removed, i.e. masked out, in order to avoid distortions in further analyses. Given that cloud masking can be interpreted as a statistical classification problem, the confusion matrix can be used to distinguish between two distinct types of errors. On the one hand, cloud omission errors (cloud as non-cloud) can lead to inconsistencies in time series of surface reflectance pixels, whereas cloud commission (clear as non-cloud) reduces the number of valid observations and, as a result, the frequency of cloud-free data (\cite{skakun2022cloud}). Cloud masking techniques are aimed to have a balance between commission and omission errors. Over the last three decades, a plethora of cloud masking methods have been presented (\cite{Hagolle2017, Domnich2021, Louis2016, Qiu2019, richter2019atmospheric, jan_wevers_2021_5788067, Lopez-Puigdollers2021, frantz2019force}). These methods can be classified into two main categories: knowledge-driven (KD) and data-driven (DD). While KD emphasizes the use of physical rules formulated on spectral and contextual features, DD is subjected to the exigency of large pixel-level annotation and costly computational requirements to distinguish cloud versus non-cloud regions.

Only a few studies have attempted to compare the various Sentinel-2 cloud masking methods. For instance, \cite{Cilli} compare DD with KD methods by analyzing 135 Sentinel-2 images distributed worldwide. They concluded that DD methods outperform KD methods. According to their experiments, $10^{4}$ manually labeled pixels are sufficient for train machine learning algorithms to operate accurately cloud masking. Nonetheless, it is well established that DD models are highly dependent on the training dataset (\cite{Lopez-Puigdollers2021}. As a result, the comparison could be unfair, especially if the KD methods have not been as well calibrated to the dataset. \cite{Zekoll2021} compare three KD threshold-based: FMask, ATCOR, and Sen2Cor using a sample-based dataset. The results show that Sen2Cor outperforms the other methods. However, human-made datasets, especially those created by sampling, can be positively skewed if we consider that humans tend to overlook unpleasant information such as cloud borders (ostrich-effect, \cite{Valdez2017}). Using four different datasets, the Cloud Mask Intercomparison eXercise (CMIX) recently compared ten cloud detection algorithms. They suggested that no single algorithm performed better than the others \cite{Skakun2022}. Similar conclusions are found in \cite{tarrio2020comparison} by analyzing 28 images over six Sentinel-2 tiles in Africa and Europe.

In this chapter, we propose to use a novel approach for exploiting the hand-crafted cloud and cloud shadow masking information freely available in cloudSEN12. Unlike previous studies, we intend not only to characterize but also to predict cloud masking uncertainty worldwide. Inspired by deep kernel learning \cite{wilson2016deep}, we explore the use of Gradient boosting Gaussian Processes (GBGP) which combines the structural properties of machine learning architectures with the non-parametric flexibility of kernel methods.

## Data

The basis for Cmask is to predict what the reflectance would be for individual pixels in a Landsat 8 image if cirrus clouds were not present at the time the observations were collected. The basis for those pre- dictions is past observations and the atmospheric conditions, i.e. the water vapor content, at the time of image acquisition. Therefore, two major inputs were included in this analysis: Integrated Water Vapor (IWV) provided by the second Modern-Era Retrospective analysis for Research and Applications (MERRA-2) and cirrus band time series provided by Landsat 8.

### cloudSEN12

CloudSEN12 is a globally spatio-temporal distributed dataset for cloud and cloud shadow semantic understanding that consists of 49,400 image patches (IP) that are evenly spread throughout all continents except Antarctica (Figure X). Each IP has an average size of 5090 x 5090 meters and contains data from Sentinel-2 optical levels 1C and 2A, Sentinel-1 Synthetic Aperture Radar (SAR), digital elevation model, surface water occurrence, land cover classes, cloud masking results from eight different algorithms and hand-crafted labeling data created using an active learning system \cite{francis_alistair_2020_4172871}. cloudSEN12 offers three different manual annotation types in order to support different deep learning strategies: (i) 10,000 IPs with high-quality pixel-level annotation, (ii) 10,000 IPs with scribble annotation, and (iii) 29,250 unlabeled IPs. Only high-quality IPs are used in this study because of the risk of bias due scene incompleteness in scribble annotation. A detailed description of the CloudSEN12 dataset is given in chapter one.

### Sen2Cor

Sentinel 2 Correction (\cite{Louis2016}) is a mono-temporal image processor designed for
scene classification and atmospheric correction of Sentinel-2 Level 1C input data. Sen2Cor version 2.8 is the version used in the Sentinel-2 ground segment with L2A processing baseline version 02.12. This processing baseline is the most recurrent version in cloudSEN12 (90% IPs). Sen2Cor (Figure X) uses a series of spectral reflectance thresholds, ratios, and indices based on bands 1–5, 8, and 10–12 to compute cloud and snow probabilities for each pixel. Besides, it includes a cloud shadow and cirrus detection algorithm. The cloud shadows are estimated by multiplying two probability layers: (1) a geometric probability layer constructed from the 
final thick cloud mask, sun position, and cloud height distribution, and (2) radiometric probability layer created from a Kohonen map to detect dark areas. On the other hand, cirrus probabilities are calculated simply by threshold band 10 (1.375 µm). Finally, a series of additional steps to improve the quality of the classification are automatically triggered using a priori information: digital elevation model (DEM) information, ESA CCI Water Bodies Map v4.0 (Lamarche et al., 2017), ESA CCI Land Cover Map v.2.0.7 (2015) and a snow climatology. In this study, SCL classes 8, 9 and 10 were used for cloud, class 3 for cloud shadow and the remaining SCL classes for non-cloud.

### Cloud type ocurrence 

The cloud type ocurrence (CTO) are derived from the CloudSat cloud profiling radar (CPR). This sensor permited for the first time to see cloud vertical structure (Stephens et al. 2008). ClouSat is part from Afternoon Constellation, or A-Train, at a frequency of 94 GHz and an altitude of about 700 km. Since 2011, it only produce daytime-only measurements due to battery malfunction, and in February 2018, the spacecraft systems moved to a lower orbit (C-train) to reduce the risk of collision with the other A-train spacecraft. The track of the satellite overpasses the same location every 16 days and provides a resolution of 1.4 km in cross-track and 1.8 km in along-track. Using the vertical profiles of clouds and precipitation, create an algorithms (\cite{}) to classify clouds according to the Cloud Climatology Project (ISCCP) approach: cumulus (Cu), stratocumulus (Sc), stratus (St), alto- cumulus (Ac), altostratus (As), nimbostratus (Ns), cirrus/ cirrostratus, or deep convective clouds. This information is storage in the 2B-CLDCLASS product. Particularly, in this study, we make use of the 3S-RMCP product of CloudSat level 3 observations. This product collocated 2B-CLDCLASS measurements in a 2.5°x2.5° grid. Based on this dataset, we constructed cloud type occurrence at 1° spatial
resolution. The TPR data used corresponds to the 2007–2016 period, the cloud type ocurrence were smoothed from 2.5° to 1° using cubic spline interpolation. We aggregathed the ISCCP classification into three classes accordin to the table x.


### MODIS


## Methodology
<!---
  Problem statement
$d-$dimensional training set of $n$ samples and real-valued targets such that $x_i\; and \; y_i \in \rm^{d}, \forall_i \; \epsilon \; \{1, ..., n\}$
  -->
  
This study aims to create a model that predicts cloud masking error in
a 1° x 1° worldwide grid system $y^*$ from a set of predictors $x^*$. The regression model $\mathbf{f}$ is trained using a dataset,  $\mathcal{D} = \{x_i, y_i\}^{N}_{i=1}$ with $x_i$ and $y_i \in \mathbb{R}$. The segment $x_i$ represents a vector of $1 \times m$ predictors (Table X), $y_i$ is the metric error values (see section \ref{section:metrics}) determined from comparing cloudSEN12 and Sen2Cor cloud masking results, and $N$ the number of observations set as 10000. Only high-quality IPs are used in this study because of the risk of bias due to scene incompleteness in scribble annotation. A detailed description of the CloudSEN12 dataset is given in chapter one. We propose the use of a deep kernel learning (DKL) regression. It combines deep neural networks (DNN) with standard gaussian process regression (GP). While ANN captures the non-stationary and hierarchical structure, GP permits the estimation of their uncertainty considering the local autocorrelation of the observations. \cite{rasmussen2003gaussian} provide a detail description of GP. The standard GP, DKL, and the approach used to create worldwide cloud error predictions are briefly reviewed in the following sections.

### Gaussian process regresion

Standard Gaussian Process Regression (GP) models is a expressive probabilistic model in which both training and testing data points are regarded samples of a joint multivariate normal distribution (\cite{williams2006gaussian}). As other regression models, a GP model is formed by noisy variables of the true underlying function $\mathbf{f}$ that projects the vector space $X$ into real-valued targets $y$, i.e. $y = f(\mathbf{x}) + \epsilon$. The element $\epsilon$ represents the noise variables with $\mathcal{N}(0, \sigma^{2})$. In a GP model we assume that all the finite dimensional distributions $f(\mathbf{x})$ are normally distributed with $\mu$ as a mean and $K_{XX}|\gamma$ as the prior covariance matrix. The covariance (kernel) matrix regulates the smoothness of GPs and its values are implicitly dependent on the kernel hyperparameters $\gamma$. In this specific case, the estimation of $f(\mathbf{x})$ can be expressed given by:
  
\begin{equation}
\begin{aligned}
\mathbf{f} = f(\mathbf{x}) = [f(x_1), ...., f(x_m)]^\top & \sim 
\mathcal{GP}\left(
  \mu, K_{XX}|\gamma\right
) \\
\end{aligned} \label{eq:1}
\end{equation}

Conditioning the joint normal distribution by the output values at the training points
($X$ and $\mathbf{y}$), the posterior distribution of the output values $f(\mathbf{x}_{*})$ at the test data point $X_*$ can be inferred as:
  
\begin{equation}
\begin{aligned}
f(\mathbf{x}_{*}) \mid X_{*}, X, \mathbf{y}, \boldsymbol{\gamma}, \sigma^{2} \sim \mathcal{N}(\mu^{*}, \Sigma^{*}), \\
\mu^{*} = \mu_{X_*} + K_{X_*X}\widehat{K}_{XX}^{-1}\mathbf{y}, \\
\Sigma^{*} = K_{X_*X_*} - K_{X_*X}\widehat{K}_{XX}^{-1}K_{XX_*}
\end{aligned} \label{eq:2}
\end{equation}

A hat denotes an added diagonal, i.e. $\widehat{K}_{XX} = K_{XX} + + \sigma^{2}I$. $\mu^{*}$ and $\Sigma^{*}$ are the posterior mean and covariance matrix respectively. The matrices of the form $K_{X_iX_j}$ denote cross-covariances between the train ($X$) and test ($X_*$) vector spaces. The hyperparameters $\lambda$ of the kernel are usually learned directly by minimizing the negative log marginal likelihood $\mathcal{L}(\theta)$ with respect to training observations:
  
\begin{equation}
\begin{aligned}
\mathcal{L} = - \log p(\mathbf{y} \mid \gamma, X) \propto \mathbf{y}^{\top} \widehat{K}_{XX}^{-1} \mathbf{y} + \log \left|\widehat{K}_{X X}\right|, \\
\frac{\partial \mathcal{L}}{\partial \theta} = \mathbf{y}^{\top} \widehat{K}_{XX} \frac{\partial \widehat{K}_{X X}^{-1}}{\partial \theta} \widehat{K}_{X X} \mathbf{y}-\operatorname{tr}\left\{\widehat{K}_{X X}^{-1} \frac{\partial \widehat{K}_{XX}}{\partial \theta}\right\}
\label{eq:3}
\end{aligned}
\end{equation}

The main bottleneck for kernel learning is solve the linear system $\widehat{K}_{XX}^{-1}y$ in equation \ref{eq:3}. The standard approach is to compute the Cholesky decomposition of the matrix $\widehat{K}_{XX}^{-1}$. The Cholesky decomposition's core algorithm 
uses a divide-and-conquer approach that is inefficient on GPU acceleration (\cite{krishnamoorthy2013matrix}). Furthermore, it requires $\mathcal{O}(n^{3})$ computation and $\mathcal{O}(n2)$ storage for GP inference and kernel learning (\cite{rasmussen2003gaussian}). To address the above challenges, several approaches to scaling up GP inference have been proposed (\cite{gardner2018gpytorch, cunningham2008fast, dong2017scalable, bach2013sharp, wilson2015thoughts}). In this paper, we address the GP inference issue by using the Blackbox Matrix-Matrix multiplication inference (BBMM, \cite{gardner2018gpytorch}). BBMM use preconditioned batched conjugate gradients to solve linear systems, reducing the asymptotic time complexity of GP inference from $\mathcal{O}(n^{3})$ to $\mathcal{O}(n^{2})$. Besides, it overcomes memory constraints by divvying the kernel matrix to perform matrix-vector multiplication (MVM, \cite{demmel1997applied}) without having to explicitly construct the kernel matrix, reducing the memory requirement to $\mathcal{O}(n)$. Finally, BBMM parallelize partitioned MVMs across multiple core, enabling a better use of GPU hardware in comparison to the Cholesky factorization. 

### Deep kernel learning

DKL is a probabilistic deep network that simultaneously learns a feature 
extractor and a Gaussian process on the feature space (cite). The network's 
structure is depicted in Figure X. The deep non-linear feature extractor 
$\mathbf{h(x,w)}$, parametrized by weights $\mathbf{w}$, is applied to the 
observed input variable $\mathbf{x}$. Next, the DNN outputs are modeled using 
$\mathcal{GP}$ by:

\begin{equation}
f(\mathbf{x}) \sim \mathcal{GP}(
    \mu(\mathbf{h_w(x)}),
    k_{\gamma}(\mathbf{h_w(x)}, \mathbf{h_w({x}')})
)
\end{equation}

All parameters of the model, including neural network weights $\mathbf{w}$ and kernel parameters $\mathbf{\gamma}$, are optimized end-to-end via backpropagation to minimize the
negative log marginal likelihood (equation \ref{eq:3}).

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/chapter02/figure02.png}
	\caption{Number of hand-crafted pixel annotations between different cloud detection datasets. All the labeled pixels in the CloudSEN12 no-annotation group come from cloud-free IPs.}
	\label{fig:figure02}
\end{figure}

### Model set-up and training

For our deep kernel learning model, we used deep neural networks which produce
C-dimensional top-level features. Here C is the number of classes. We place a Gaussian process on each dimension of these features. We used RBF base kernels. The additive GP layer is then followed by a linear mixing layer A  R C×C . We initialized A to be an identity matrix, and optimized in the joint learning procedure to recover cross-dimension correlations from data.
We first train a deep neural network using SGD with the softmax loss objective, and rectified linear activation functions. After the neural network has been pre-trained, we fit an additive KISS-GP layer, followed by a linear mixing layer, using the top-level features of the deep network as inputs. Using this pre-training initialization, our joint SV-DKL model of section 3 is then trained through the stochastic variational method of section 4 which jointly optimizes all the hyperparameters  of the deep kernel (including all network weights), as well as the variational parameters, by backpropagating derivatives through the proposed marginal likelihood lower bound of the additive Gaussian process in section 4. In all experiments, we use a relatively large mini-batch size (specified according to the full data size), enabled by the proposed structure exploiting variational inference procedures. We achieve good performance setting the number of samples T = 1 in Eq. 4 for expectation estimation in variational inference, which provides additional confirmation for a similar observation in [14].

### Model metrics 
\label{section:metrics}

### Model predictions


## Experimental results


## Discussion

dsada 

## Conclusion

dsada 